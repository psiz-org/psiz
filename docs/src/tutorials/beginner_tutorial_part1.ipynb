{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "timeout": -1
    }
   },
   "source": [
    "# Beginner Tutorial - Part 1\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The Beginner Tutorial Part 1 uses simulated (fake) data in order to keep things simple (see Part 2 for an application that uses real data). The tutorial covers the path from problem formulation, data preparation, model inference, and model analysis. The tutorial aims to teach the basic organization and mechanics of PsiZ, giving readers the background necessary to use PsiZ in their own projects. Some familiarity with TensorFlow is assumed. In order to provide a relatively gentle introduction, this tutorial bypasses some best practices and more advanced features.\n",
    "\n",
    "```{note}\n",
    "When a short-cut is taken, a callout box like this one highlights the best practice for real applications.\n",
    "```\n",
    "\n",
    "Big picture, PsiZ uses observable human behavior (such as similarity judgments) to infer psychological representations people use to perceive the world. While this tutorial uses fake stimuli, PsiZ is designed to model psychological representations of the real-world: such as images, audio, movies, words, odors, and text. After completing this tutorial you should have a good idea how to use PsiZ with your own data.\n",
    "\n",
    "The tutorial is divided into five parts:\n",
    "\n",
    "1. Problem Formulation\n",
    "2. Model Construction\n",
    "2. Data Preparation\n",
    "4. Model Inference\n",
    "5. Model Analysis\n",
    "\n",
    "If you would like to run this notebook on your local machine, the file is available at [PsiZ's GitHub](https://github.com/psiz-org/psiz/blob/main/docs/src/beginner_tutorial/beginner_tutorial_part1.ipynb)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminaries\n",
    "\n",
    "Start by importing packages and setting some environment variables. We also reduce TensorFlow's log output to a quieter setting.\n",
    "\n",
    "```{note}\n",
    "If you want to force execution to use a specific GPU, you can uncomment the lines marked with an `A`. If uncommented, the current setting will force execution to use \"GPU 0\". If you want to force *eager execution*, uncomment the line marked with a `B`. Eager execution is useful for debugging since it uses a non-optimized CPU mode that allows line-by-line state inspection. See the [TensorFlow Guide](https://www.tensorflow.org/guide/effective_tf2#debugging) for more information.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "import psiz\n",
    "\n",
    "# Optional settings, see Note for explanation.\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"  # A\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # A\n",
    "# tf.config.run_functions_eagerly(True)  # B"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Formulation\n",
    "\n",
    "Imagine we are interested in understanding how the average person perceives a set of **30 stimuli**. These 30 stimuli could be almost anything: bird images, music tracks, beer flavors, flower fragrences, haikus.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We would like to assemble a cognitive map that reveals how the stimuli are arranged in psychological space. Being able to uncover the mental representations used by people has important applications in society. For example, it could help researchers understand how medical professionals recognize which skin lesions are life threatening. In turn, that knowledge could be leveraged to make medical training more efficient and less effortful.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Solving with PsiZ\n",
    "\n",
    "PsiZ uses formal models to understand mental representations. Since mental representations are not directly observable, we use statistical inference to identify the psychological representations that are most likely to explain observable behavior. A PsiZ model has two conceptual stages: (1) mapping a stimulus to a psychological representation (i.e., into a psychological space) and (2) mapping psychological representations to observable behavior. Each of these stages can be arbitrarily complex, but at a minimum we expect at least three model components: a *percept* module, *proximity* module, and *behavior* module ({numref}`fig-model-flow-1`). The percept module and proximity module work hand-in-hand to map a stimulus to a psychological representation. The behavior module details how the psychological representations yield the observed behavior. These modules are discussed in more detail below when we construct our first model.\n",
    "\n",
    "```{figure} ../../img/model_flow_1task_v4.png\n",
    "---\n",
    "align: center\n",
    "width: 350px\n",
    "name: fig-model-flow-1\n",
    "alt: \"Stimulus and agent inputs feeding into large box denoting a cognitive model which outputs behavioral predictions. The large box has two smaller boxes denoting the two major modules: a percept and behavior module.\"\n",
    "---\n",
    "High-level organization of a generic PsiZ model. Stimulus and agent information is provided as input to a cognitive model. After internal processing, the model outputs the probability associated with different behavioral outcomes. Stimulus information includes details about *content* (e.g., information about the images shown on a trial). Agent information includes details about the human or computer agent (e.g., a personal identifier, experimental condition). The \"wires\" in the diagram show the flow of information through the model; from inputs, to percept module, to proximity module, to behavior module, to outputs. Agent (yellow) and stimulus (blue) information can influence the percept, proximity and behavior modules.\n",
    "```\n",
    "\n",
    "```{note}\n",
    "Since a model can be run in generative mode, the model can be treated as *virtual subject*. For this reason, it is often preferable to use the more general term *agent*, which includes both human and machine.\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Choosing a Task\n",
    "\n",
    "Many tasks can be used to ellicit obserable behavior, but a handful of tasks are particularly popular for probing psychological representations. Popular tasks include:\n",
    "\n",
    "* **pairwise rating**: given two stimuli, rate the similarity of the two stimuli on a numerical scale (e.g., 1 to 7)\n",
    "* **triplet similarity**: given 1 stimulus, choose which of two additional stimuli is most similar\n",
    "* **odd-one-out**: given three images, select the image that doesn't belong\n",
    "* **categorization**: given an image, provide the category label.\n",
    "\n",
    "Some tasks will be better suited for certain stimuli. For example, if judging the similarity between odors, odors will need to be presented separately to prevent the odors from mixing; this makes sequential pairwise ratings a good task. In contrast, if using images as stimuli, multiple images can be shown simultaneously by tiling them on the display; allowing for a wider variety of tasks.\n",
    "\n",
    "**In this tutorial, we use the triplet similarity task.** In a triplet similarity task, one stimulus serves as a *query* and two stimuli serve as *references*. The participant is tasked with selecting the reference that is most similarity to the query. A triplet similarity task is a special case of a more general *rank similarity* task. In a rank similarity task, a participant is shown `n_reference` references and must make `n_select` selections while also indicating the ranking of their selections. For the remainder of the tutorial we will refer to the triplet similarity task as a *2-rank-1* task."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Hyperparameters\n",
    "\n",
    "The simplest percept module is an embedding layer which maps a stimulus ID to a multidimensional representation (i.e., an embedding point). A common hyperparameter for embedding layers is the number of dimensions of the embedding space (what `tf.keras.layers.Embedding` calls `output_dim`). For this tutorial we will infer a **two-dimensional embedding** in order to keep the model simple and facilitate later visualization.\n",
    "\n",
    "```{note}\n",
    "When inferring an embedding in a real application, it is typically important to treat the dimensionality as a hyperparameter and select in an appropriate manner (e.g., via cross-validation).\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the Number of Trials\n",
    "\n",
    "Each collected 2-rank-1 corresponds to a *triplet constraint*. Consider a trial with query stimulus $q$ and reference stimuli $a$ and $b$. If a participant selects $a$ as more similar, we can use a shorthand to denote the the triplet constraint $a > b | q$.\n",
    "\n",
    "Inference is a search for a solution that best satisfies all the triplet constraints. In a multidimensional solution, stimuli that are judged as similar are placed close together in the multidimensional space and stimuli judged as dissimilar are placed far apart. We refer to this kind of representation as a *psychological embedding*.\n",
    "\n",
    "If you give the same trial to different people, you may get different responses (i.e., between-subject variability). The same person might even give different reponses for the same trial, at different points in time (i.e., within-subject variability). The inference procedure will find the embedding that best satisfies all the (potentially noisy) triplet constraints.\n",
    "\n",
    "It is difficult to predict how many trials will be necessary to obtain a high-quality embedding. Intuitively, you need enough triplet constraints to rule out incorrect solutions. There are three main factors that influence this amount:\n",
    "\n",
    "1. More trials are necessary if the number of stimuli increases.\n",
    "2. More trials are necessary if within- or between-subject agreement decreases.\n",
    "3. The trial content selection strategy impacts how many trials are necessary.\n",
    "\n",
    "This tutorial side-steps this issue by collecting a **pre-determined number of trials**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```{note}\n",
    "While predicting the necessary number of trials is challenging, it is relatively  straightforward to perform a post hoc evaluation that tests if a sufficient amount of data has been collected. For more details, see examples/rank/mle_1g.py and tutorials/beginner_tutorial_part_2\n",
    "```\n",
    "\n",
    "```{note}\n",
    "In regards to different content selection strategies, using active learning to select trial content instead of randomly sampling trials can greatly reduce the number of trials necessary to achieve a stable embedding. See {cite}`Roads_Mozer_2019_BRM` and {cite}`Roads_Love_2021:CVPR` for details on performing active learning.\n",
    "```\n",
    "\n",
    "Collecting all of our problem specifications gives us the following variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of stimuli.\n",
    "n_stimuli = 30\n",
    "\n",
    "# Settings defining the 2-rank-1 (triplet) similarity task.\n",
    "n_reference = 2\n",
    "n_select = 1\n",
    "\n",
    "# Hyperparameter choices.\n",
    "n_dim = 2\n",
    "\n",
    "# Number of trials to use.\n",
    "n_trial = 3000"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "With the general problem parameters specified, we can dive into constructing an appropriate model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Construction\n",
    "\n",
    "A PsiZ model contains at least three model components ({numref}`fig-model_2rank1_example`). These components are intentionally denoted as *modules* because each can be substituted with arbitrarily complex variants.\n",
    "\n",
    "1. A *percept* module describes the data structure used to model a mental representation. For example, a multidimensioanl vector, set, or graph.\n",
    "2. A *proximity* module describes how proximity is computed between two psychological representations. For example, this could be Euclidean distance, cosine similarity, or something more sophisticated. The term *proximity* is intentional, because depending on the setup a research may prefer working with dissimilarity values or similarity values. Together, a percept module and proximity module define a psychological space.\n",
    "3. A *behavior* module describes how the perceived stimuli yield the observed task behaviour. A given model may have more than one behavior module in order to predict multiple types of behavior.\n",
    "\n",
    "```{figure} ../../img/model_2rank1_example_v5.png\n",
    "---\n",
    "align: center\n",
    "name: fig-model_2rank1_example\n",
    "alt: \"A flow diagram showing inputs feeding to an embedding percept module, a proximity module, a 2-rank-1 similarity behavior module, and then producing behavioral outputs.\"\n",
    "---\n",
    "A diagram of the generative model used in this tutorial. The diagram is simplified by assuming there are only 10 stimuli, although the tutorial uses 30 stimuli.\n",
    "```\n",
    "\n",
    "```{note}\n",
    "In TensorFlow, a *layer* is the name used for an abstract building block of a model. When using TensorFlow it is best practice to assemble your models from layers. One of the main benefits of PsiZ is the set of layers it provides.\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Percept Module\n",
    "\n",
    "The percept module formalizes the notion of a mental representation by adopting a particular data structure. In this tutorial, the percept module is a *population-level* embedding layer where each perceived stimulus is modeled as a multidimensional vector.\n",
    "\n",
    "```{note}\n",
    "A population-level layer means that the free parameters are fit to all of the data and ignore agent membership.\n",
    "```\n",
    "\n",
    "```\n",
    "percept = tf.keras.layers.Embedding(\n",
    "    n_stimuli + 1,\n",
    "    n_dim,\n",
    "    mask_zero=True,\n",
    "    embeddings_initializer=tf.keras.initializers.RandomNormal(stddev=.17)\n",
    ")\n",
    "```\n",
    "\n",
    "The embedding coordinates are assigned to the layer variable `percept`. If we assume that the embedding has $N$ stimuli and $D$ dimensions, then the `percept` layer has a learnable weight matrix $\\bm{Z} \\in \\mathbb{R}^{N \\times D}$. The weight matrix $\\bm{Z}$ behaves like a lookup table during a forward pass of the model. The weight matrix $\\bm{Z}$ is randomly initialized according to a Normal distribution.\n",
    "\n",
    "For example, consider the case where a 2-rank-1 trial is presented to the model (the case shown in {numref}`fig-model_2rank1_example`). If stimulus `9` is the query and stimuli `1` and `7` are references (i.e., $\\bm{x}_i = \\left[9, 1, 7 \\right]$), we would grab the corresponding rows of $\\bm{Z}$: $\\bm{z}_9$, $\\bm{z}_1$, and $\\bm{z}_7$. Each row, or vector, represents the corresponding $d$-dimensional coordinates of stimuli `9`, `1`, and `7`.\n",
    "\n",
    "We use the `mask_zero` argument to let the model know it should ignore `0` placeholder integers. While ignoring `0` is not necessary in this tutorial, we do it to stay consistent with other tutorials and examples where it is necessary.\n",
    "\n",
    "```{note}\n",
    "If you want to model individual differences or differences between experimental conditions, you could use a more sophisticated percept module. For example, you could use an implementation based on the classic idea of *attention weights* (see [`examples/rank/mle_3g.py`](https://github.com/psiz-org/psiz/blob/main/examples/rank/mle_3g.py)) or use more than one embedding layer.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proximity Module\n",
    "\n",
    "The proximity module formalizes how an agent perceives similarity (or dissimilarity) between two stimuli. In this tutorial, the proxmity module is a population-level layer that computes a nonlinear transformation of Euclidean distance.\n",
    "\n",
    "```\n",
    "proximity = psiz.keras.layers.Minkowski(\n",
    "    rho_initializer=tf.keras.initializers.Constant(2.),\n",
    "    w_initializer=tf.keras.initializers.Constant(1.),\n",
    "    activation=psiz.keras.layers.ExponentialSimilarity(\n",
    "        beta_initializer=tf.keras.initializers.Constant(10.),\n",
    "        tau_initializer=tf.keras.initializers.Constant(1.),\n",
    "        gamma_initializer=tf.keras.initializers.Constant(0.),\n",
    "        trainable=False,\n",
    "    ),\n",
    "    trainable=False\n",
    ")\n",
    "```\n",
    "\n",
    "The proximity layer is assigned to the variable `proximity`. The proximity layer `psiz.keras.layers.Minkowski` is itself composed of two steps: the core Minkowski distance computation and an (optional) transformation of the distance value (`psiz.keras.layers.ExponentialSimilarity`). It is worth noting that both of these steps are parameterized and the parameters can be learned from the data. To keep the tutorial simple, all of the proximity parameters are set to sensible values and made untrainable via the `trainable=False` argument.\n",
    "\n",
    "The `Minkowski` layer computes the weighted Minkowski distance between two vectors:\n",
    "```{math}\n",
    ":label: eq:minkowski\n",
    "\\mathrm{d} \\left( \\bm{z}_i, \\bm{z}_j \\right) = \\| \\bm{z}_i - \\bm{z}_j\\|_{\\rho, \\bm{w}} = \\left[ \\sum^{D}_{d} w_{d} \\left( z_{i,d} - z_{j,d}\\right)^{\\rho} \\right]^{\\frac{1}{\\rho}}.\n",
    "```\n",
    "The distance function parameters are set to $\\rho=2$ and $\\bm{w}=\\bm{1}$, which yields the standard Euclidean distance function. The `ExponentialSimilarity` layer takes the distance and applies a non-linear mapping:\n",
    "```{math}\n",
    ":label: eq:similarity\n",
    "\\mathrm{s} \\left( \\bm{z}_i, \\bm{z}_j \\right) = \\mathrm{exp}\\left(-\\beta \\mathrm{d}\\left( \\bm{z}_i, \\bm{z}_j \\right)^{\\tau} \\right) + \\gamma.\n",
    "```\n",
    "The activation function parameters are set to $\\beta=10$, $\\gamma=0$, and $\\tau=1$; yielding similarity that decays exponentially as distance increases.\n",
    "\n",
    "The psychological theory motivating this proximity function can be found elsewhere {cite}`Jones_Love_Maddox_2006,Jones_Maddox_Love_2006,Nosofsky_1986,Roads_Mozer_2019_BRM,Shepard_1987`, but it is worth noting that many other proximity functions are possible. Decomposing the proximity function into two steps makes it easy to explore different variants. For example, one might want to try a heavy-tailed activation function that is popular in the machine learning literature {cite}`vanderMaaten_2012:MLSP` or use a non-Minkowski distance function.\n",
    "\n",
    "Continuing with our 2-rank-1 example, the coordinates ($\\bm{z}_9$, $\\bm{z}_1$, $\\bm{z}_9$) are used in the proximity function to compute similarity. A 2-rank-1 trial involves computing two similarity relations: $\\mathrm{s}\\left( \\bm{z}_9, \\bm{z}_1 \\right)$ and $\\mathrm{s}\\left( \\bm{z}_9, \\bm{z}_1 \\right)$.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Behavior Module\n",
    "\n",
    "A behavior module maps psychological representations to an observed behavior. In this tutorial, there is one population-level behavior module that models 2-rank-1 (i.e., triplet) similarity judgments. The behavioral outcomes of a 2-rank-1 trial are computed using a `SoftRank` layer.\n",
    "\n",
    "```\n",
    "soft_2rank1 = psiz.keras.layers.SoftRank(n_select=n_select)\n",
    "```\n",
    "\n",
    "The layer name is partially inspired by the naming of a \"softmax\" operation. Instead of outputing probabilites associated with particular items, it outputs probabilities associated with different ranking outcomes. The unique ranking outcomes are determined by the number of options and the number of selections(`n_select`). The number of options is inferred from the input shape. It is therefore a good idea to name the layer in a way that makes its intended use clear (e.g., `soft_2rank1`). Outcomes probabilities are computed using Luce's choice rule {cite}`Luce_1959`. In the case of a 2-rank-1 trial, a reference is selected in proportion to its similarity with the query. The probability of selecting reference `a` instead of reference `b` is:\n",
    "```{math}\n",
    ":label: eq:2-rank-1\n",
    "\\mathrm{P} \\left( \\mathcal{D}_{i} = \\left[q, a, b \\right] \\right) = \\frac{\\mathrm{s} \\left( \\bm{z}_q, \\bm{z}_a \\right)}{\\mathrm{s} \\left( \\bm{z}_q, \\bm{z}_a \\right) + \\mathrm{s} \\left( \\bm{z}_q, \\bm{z}_b \\right)}.\n",
    "```\n",
    "\n",
    "Continuing with the example introduced above, given query `9`, the probabilty of selecting reference `1` (instead of `7`) is:\n",
    "```{math}\n",
    ":label: eq:2-rank-1-a\n",
    "\\mathrm{P} \\left( \\mathcal{D}_{i} = \\left[9, 1, 7 \\right] \\right) = \\frac{\\mathrm{s} \\left( \\bm{z}_9, \\bm{z}_1 \\right)}{\\mathrm{s} \\left( \\bm{z}_9, \\bm{z}_1 \\right) + \\mathrm{s} \\left( \\bm{z}_9, \\bm{z}_7 \\right)}.\n",
    "```\n",
    "\n",
    "Using this equation you can compute the probabilities associated with all possible outcomes. For a 2-rank-1 trial there are only two possible outcomes. If one needs to run a model in generative mode, these probabilities can be converted to a single outcome by sampling from the probabilities."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting It All Together\n",
    "\n",
    "The last step is to package all of these pieces up inside a Keras `Model`. To make it easy to build and compile multiple models, we wrap the construction code inside a `build_rank_model` function. The compile step uses categorical crossentropy for the loss (i.e., maximum likelihood estimation).\n",
    "\n",
    "```{note}\n",
    "Maximum likelihood estimation (MLE) is a convenient method, but is limited by the fact it lacks a direct read-out of confidence (i.e., uncertainty). In situations where you also need to know how certain an estimate is, you could use an approach like variational inference (see [`examples/rank/vi_1g.py`](https://github.com/psiz-org/psiz/blob/main/examples/rank/vi_1g.py)).\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RankModel(tf.keras.Model):\n",
    "    \"\"\"A Keras model of ranked similarity judgments.\"\"\"\n",
    "\n",
    "    def __init__(self, percept=None, proximity=None, **kwargs):\n",
    "        \"\"\"Initialize.\"\"\"\n",
    "        super(RankModel, self).__init__(**kwargs)\n",
    "        # NOTE: The `stimuli_axis` is the axis` used to store indices of\n",
    "        # all the quries and references.\n",
    "        self.stimuli_axis = 1\n",
    "\n",
    "        self.percept = percept\n",
    "        self.proximity = proximity\n",
    "\n",
    "        # Create a behavior module for the 2-rank-1 similarity task.\n",
    "        self.soft_2rank1 = psiz.keras.layers.SoftRank(n_select=n_select)\n",
    "\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"Call.\"\"\"\n",
    "        z = self.percept(inputs[\"given2rank1_stimulus_set\"])\n",
    "        z_q, z_r = tf.split(z, [1, 2], self.stimuli_axis)\n",
    "        s = self.proximity([z_q, z_r])\n",
    "        return self.soft_2rank1(s)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super(RankModel, self).get_config()\n",
    "        return config\n",
    "\n",
    "\n",
    "def build_rank_model(n_stimuli, n_reference, n_select, n_dim):\n",
    "    \"\"\"Build ranked similarity model.\n",
    "\n",
    "    Args:\n",
    "        n_stimuli: Integer indicating the number of stimuli in the\n",
    "            embedding.\n",
    "        n_reference: Integer indicating the number of references in\n",
    "            the ranked similarity judgment.\n",
    "        n_select: Integer indicating the number of selections in the\n",
    "            ranked similarity judgment.\n",
    "        n_dim: Integer indicating the dimensionality of the embedding.\n",
    "\n",
    "    Returns:\n",
    "        model: A compiled Keras model.\n",
    "\n",
    "    \"\"\"\n",
    "    # Create a psychological embedding layer that contains free parameters\n",
    "    # representing the psychological coordinates of the stimuli.\n",
    "    percept = tf.keras.layers.Embedding(\n",
    "        n_stimuli + 1,\n",
    "        n_dim,\n",
    "        mask_zero=True,\n",
    "        embeddings_initializer=tf.keras.initializers.RandomNormal(stddev=.17)\n",
    "    )\n",
    "\n",
    "    # Create a proximity layer that computes the similarity between\n",
    "    # coordinates in psychological space.\n",
    "    proximity = psiz.keras.layers.Minkowski(\n",
    "        rho_initializer=tf.keras.initializers.Constant(2.),\n",
    "        w_initializer=tf.keras.initializers.Constant(1.),\n",
    "        activation=psiz.keras.layers.ExponentialSimilarity(\n",
    "            beta_initializer=tf.keras.initializers.Constant(10.),\n",
    "            tau_initializer=tf.keras.initializers.Constant(1.),\n",
    "            gamma_initializer=tf.keras.initializers.Constant(0.),\n",
    "            trainable=False,\n",
    "        ),\n",
    "        trainable=False\n",
    "    )\n",
    "    \n",
    "    # Pass component layers to the subclassed Keras model.\n",
    "    model = RankModel(percept=percept, proximity=proximity)\n",
    "\n",
    "    # Compile the model using categorical crossentropy for the loss and\n",
    "    # a popular gradient decent optimizer.\n",
    "    model.compile(\n",
    "        loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=.001),\n",
    "        metrics=[tf.keras.metrics.CategoricalAccuracy(name='accuracy')]\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "Since we are using simulated data, our data preparation pipeline will include a virtual agent. We will treat the virtual agent as our *ground truth* model. Our hope is that the *inferred* model (which is blind to the embedding coordinates of the ground truth model) learns an embedding that   matches the ground truth model.\n",
    "\n",
    "We will start by creating our virtual agent (ground truth model) that outputs 2-rank-1 predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "virtual_agent_2rank1 = build_rank_model(\n",
    "    n_stimuli, n_reference, n_select, n_dim\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we generate a random sample of 2-rank-1 trial content and format as a `tf.data.Dataset` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a random set of trials.\n",
    "rng = np.random.default_rng()\n",
    "eligibile_indices = np.arange(n_stimuli) + 1\n",
    "p = np.ones_like(eligibile_indices) / len(eligibile_indices)\n",
    "stimulus_set = psiz.utils.choice_wo_replace(\n",
    "    eligibile_indices, (n_trial, 3), p, rng=rng\n",
    ")\n",
    "content = psiz.data.Rank(stimulus_set, n_select=1)\n",
    "pds = psiz.data.Dataset([content])\n",
    "tfds_content = pds.export(export_format='tfds')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{note}\n",
    "You may have noticed that the stimulus IDs begin at `1`, not `0`. This ties back to our choice to use `mask_zero=True`.\n",
    "```\n",
    "\n",
    "```{note}\n",
    "Since we are using the TensorFlow ecosystem, we convert the data objects into `tf.data.Dataset` objects. This allows us to seamlessly exploit TF's flexible data processing pipeline. You can learn more by going to [TF's tf.data.Dataset documentation](https://www.tensorflow.org/api_docs/python/tf/data/Dataset).\n",
    "```\n",
    "\n",
    "The last data preparation step is to specify our batch settings.\n",
    "\n",
    "```{note}\n",
    "The `batch_size` is typically chosen to be a power of 2 since this tends to be more efficient for reasons related to computer architecture. In memory-intensive applications, one typically picks the largest possible power of 2 such that the memory footprint of the batch will still fit on the GPUs/TPUs being used. However, you do not want a batch size that is so large that there is only one batch because multiple batches adds noise that helps gradient decent avoid local optimums (an important theoretical properity of stochastic gradient decent). In this application, our memory-footprint is small and a wide range of values work well.\n",
    "```\n",
    "\n",
    "We then feed the trial content to the virtual agent and simulate trial behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "# Simulate similarity judgments and append outcomes to dataset.\n",
    "depth = content.n_outcome\n",
    "\n",
    "def simulate_agent(x):\n",
    "    # Compute probability associated with all possible outcomes (2 outcomes).\n",
    "    outcome_probs = virtual_agent_2rank1(x)\n",
    "    # Sample an outcome based on probabilities.\n",
    "    outcome_distribution = tfp.distributions.Categorical(\n",
    "        probs=outcome_probs\n",
    "    )\n",
    "    outcome_idx = outcome_distribution.sample()\n",
    "    # Convert sampled outcome into a one-hot vector.\n",
    "    outcome_one_hot = tf.one_hot(outcome_idx, depth)\n",
    "    return outcome_one_hot\n",
    "\n",
    "tfds_content = tfds_content.batch(batch_size=batch_size, drop_remainder=False)\n",
    "tfds_all = tfds_content.map(lambda x: (x, simulate_agent(x))).cache()\n",
    "tfds_all = tfds_all.unbatch()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, when fitting a model we are primarily intersted in how well a model generalizes to **new** data and therefore want to prevent overfitting. To achive this goal we follow standard practice and partition the data into a train, validation, and test set. In this tutorial we use an 80:10:10 split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partition the data into 80% train, 10% validation and 10% test set.\n",
    "n_trial_train = int(np.floor(0.8 * n_trial))\n",
    "n_trial_val = int(np.floor(0.1 * n_trial))\n",
    "\n",
    "tfds_train = tfds_all.take(n_trial_train).cache()\n",
    "tfds_valtest = tfds_all.skip(n_trial_train)\n",
    "tfds_val = tfds_valtest.take(n_trial_val).cache()\n",
    "tfds_test = tfds_valtest.skip(n_trial_val).cache()\n",
    "\n",
    "tfds_train = tfds_train.shuffle(\n",
    "    buffer_size=n_trial_train, reshuffle_each_iteration=True\n",
    ").batch(\n",
    "    batch_size, drop_remainder=False\n",
    ")\n",
    "tfds_val = tfds_val.batch(\n",
    "    batch_size, drop_remainder=False\n",
    ")\n",
    "tfds_test = tfds_test.batch(\n",
    "    batch_size, drop_remainder=False\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our data ready, we can move on to inferring a model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Inference\n",
    "\n",
    "With our data ready, we create a fresh model that will serve as our *inferred* model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "inferred_model = build_rank_model(n_stimuli, n_reference, n_select, n_dim)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train our models until validation loss stops improving. This is implemented using an `EarlyStopping` callback.\n",
    "\n",
    "The number of epochs (`epochs`) is set to a sufficiently high number that the model reaches the early stopping criteria before the maximum number of epochs is exceeded. For our loss, we use categorical crossentropy because we are computing the probability of all possible outcomes (a categorical output).\n",
    "\n",
    "```{note}\n",
    "Since PsiZ carefully adheres to TensorFlow and Keras idioms, we can easily use features like Callbacks without any headaches. See TF documentation for more on [TF Callbacks](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/Callback).\n",
    "```\n",
    "\n",
    "```{note}\n",
    "Inference can take a few minutes to run depending on the hardware you are using.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferred Model\n",
      "Metrics\n",
      "          | loss  | acc.  |\n",
      "    train | 0.440 | 0.785 |\n",
      "    val   | 0.499 | 0.777 |\n",
      "    test  | 0.501 | 0.763 |\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def train_routine(model, tfds_train, tfds_val, tfds_test):\n",
    "    \"\"\"Run training routine.\n",
    "    \n",
    "    Args:\n",
    "        model: A PsiZ model.\n",
    "        tfds_train: A TF Dataset object of train data.\n",
    "        tfds_val: A TF Dataset object of validation data.\n",
    "        tfds_test: A TF Dataset object of test data.\n",
    "\n",
    "    Returns:\n",
    "        metrics: A dictionary of metrics.\n",
    "\n",
    "    \"\"\"\n",
    "    epochs = 3000\n",
    "\n",
    "    # Define an early stopping callback.\n",
    "    early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "        'val_loss', patience=30, mode='min', restore_best_weights=True\n",
    "    )\n",
    "    callbacks = [early_stop]\n",
    "\n",
    "    model.fit(\n",
    "        x=tfds_train, validation_data=tfds_val, epochs=epochs,\n",
    "        callbacks=callbacks, verbose=0\n",
    "    )\n",
    "\n",
    "    train_results = model.evaluate(\n",
    "        tfds_train, verbose=0, return_dict=True\n",
    "    )\n",
    "    val_results = model.evaluate(\n",
    "        tfds_val, verbose=0, return_dict=True\n",
    "    )\n",
    "    test_results = model.evaluate(\n",
    "        tfds_test, verbose=0, return_dict=True\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        'Metrics\\n'\n",
    "        '          | loss  | acc.  |\\n'\n",
    "        '    train | {0:.3f} | {1:.3f} |\\n'\n",
    "        '    val   | {2:.3f} | {3:.3f} |\\n'\n",
    "        '    test  | {4:.3f} | {5:.3f} |\\n'.format(\n",
    "            train_results['loss'],\n",
    "            train_results['accuracy'],\n",
    "            val_results['loss'],\n",
    "            val_results['accuracy'],\n",
    "            test_results['loss'],\n",
    "            test_results['accuracy'],\n",
    "        )\n",
    "    )\n",
    "    results = {\n",
    "        'train': train_results,\n",
    "        'val': val_results,\n",
    "        'test': test_results,\n",
    "    }\n",
    "    return results\n",
    "\n",
    "print('Inferred Model')\n",
    "results = train_routine(inferred_model, tfds_train, tfds_val, tfds_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results from training reveal reasonable agreement between the train, validation, and test set. Note that the accuracy is less that `1.0`, but greter than chance (`0.5`).  The optimization process attempts to find the solution that satisfies as many triplet constraints as possible. Since the agent data was generated using a stochastic process, some of the simulated behavior is inconsistent. Inconsistent data is a common occurrence when using data from multiple people."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Analysis\n",
    "\n",
    "Model analysis is typically a multi-pronged approach. We will start by visualizing the embeddings and then detail a popular quantitative approach for comparing embeddings."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Visualization\n",
    "\n",
    "One way to explore the quality of the learned model is to visualize the entire embedding. While this is not always possible with high dimensional embeddings, our 2D embeddings are easy to visualize. We will use `matplotlib` and color code the 30 stimuli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import numpy as np\n",
    "\n",
    "def color_styling(n_stimuli):\n",
    "    \"\"\"Hard-coded styling.\n",
    "    \n",
    "    First ten stimuli get colorful styling, remaining stimuli are gray.\n",
    "\n",
    "    Args:\n",
    "        n_stimuli: Integer indicating the number of stimuli.\n",
    "\n",
    "    Returns:\n",
    "        color_arr: An array of colors.\n",
    "\n",
    "    \"\"\"\n",
    "    # Define colors.\n",
    "    n_color = 10\n",
    "    cmap = matplotlib.cm.get_cmap('gist_ncar')\n",
    "    norm = matplotlib.colors.Normalize(vmin=0., vmax=n_color)\n",
    "    color_arr = cmap(norm(range(n_color)))\n",
    "\n",
    "    n_gray = n_stimuli - n_color\n",
    "    gray_arr = .7 * np.ones([n_gray, 4])\n",
    "    gray_arr[:, 3] = 1.\n",
    "\n",
    "    color_arr =  np.vstack([color_arr, gray_arr])\n",
    "    return color_arr\n",
    "\n",
    "color_arr = color_styling(n_stimuli)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our styling defined, we can create a visualization for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_embedding_overlap(ax, agent, inferred, color_arr):\n",
    "    \"\"\"Plot embedding.\n",
    "    \n",
    "    Args:\n",
    "        ax: A Matplotlib axes object.\n",
    "        agent: A Keras model.\n",
    "        inferred: A Keras model.\n",
    "        class_arr: An array of colors.\n",
    "\n",
    "    \"\"\"\n",
    "    # Hardcoded values.\n",
    "    s = 10  # Size of plot markers.\n",
    "    scatter_padding = 1.2  # Padding around embedding scatter points.\n",
    "\n",
    "    loc_agent = process_coordinates(agent)\n",
    "    loc_inferred = process_coordinates(inferred)\n",
    "\n",
    "    # Apply Procrustes transformation since our embeddings are rotation\n",
    "    # invariant.\n",
    "    r = psiz.utils.procrustes_rotation(\n",
    "        loc_inferred, loc_agent, scale=True\n",
    "    )\n",
    "    loc_inferred = np.matmul(loc_inferred, r)\n",
    "\n",
    "    # Determine embedding limits of family.\n",
    "    z_max = scatter_padding * np.max(np.abs(loc_agent))\n",
    "    z_limits = np.array([-z_max, z_max])\n",
    "\n",
    "    # Draw embedding points.\n",
    "    ax.scatter(\n",
    "        loc_agent[:, 0],\n",
    "        loc_agent[:, 1],\n",
    "        s=s,\n",
    "        c='none',\n",
    "        marker='s',\n",
    "        edgecolors=color_arr,\n",
    "        label='Agent'\n",
    "    )\n",
    "    ax.scatter(\n",
    "        loc_inferred[:, 0],\n",
    "        loc_inferred[:, 1],\n",
    "        s=s,\n",
    "        c=color_arr,\n",
    "        marker='o',\n",
    "        edgecolors='none',\n",
    "        label='Inferred'\n",
    "    )\n",
    "    ax.set_xlim(z_limits)\n",
    "    ax.set_ylim(z_limits)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "\n",
    "def process_coordinates(model):\n",
    "    # Grab embedding coordinates.\n",
    "    loc = model.percept.embeddings.numpy()\n",
    "    if model.percept.mask_zero:\n",
    "        # Drop placeholder stimulus.\n",
    "        loc = loc[1:]\n",
    "\n",
    "    # Center coordinates for Procrustes and plotting.\n",
    "    loc = loc - np.mean(loc, axis=0, keepdims=True)\n",
    "\n",
    "    return loc\n",
    "\n",
    "\n",
    "# Create visual of two models.\n",
    "fig = plt.figure(figsize=(3, 4), dpi=200)\n",
    "gs = fig.add_gridspec(1, 1)\n",
    "\n",
    "ax = fig.add_subplot(gs[0, 0])\n",
    "plot_embedding_overlap(\n",
    "    ax, virtual_agent_2rank1, inferred_model, color_arr\n",
    ")\n",
    "ax.set_title('Embedding Overlap')\n",
    "ax.legend()\n",
    "\n",
    "gs.tight_layout(fig)\n",
    "plt.plot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What about the differences between the two embeddings?\n",
    "\n",
    "Some of the coordinates for the inferred embedding do not overlap with the corresponding coordinates of the virtual agent. We have already confirmed that the two models have similar losses, so what's going on? While this difference may seem meaningful from the perspective of L2 (Eulcidean) distance, it is important to remember that similarity (not distance) is the critical quantity in these models. In these models, similarity decays exponentially with increasing distance. This means that two stimuli that are moderately far apart and extremely far apart will both have a small similarity. As a consequence there are a large number of equally good solutions for embedding dissimilar items. In contrast, the relatively similar items constrain which solutions are viable. This flexibility means that visual inspection is fine for initial exploration, but it is not a great way to *compare* embeddings. To make comparisons, we need an alternative strategy."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second-Order Isomporphism\n",
    "\n",
    "Embedding coordinate visualizations yield figures that provide a general picture of the learned solution. But coordinates only tell part of the story. It is more important that the emeddings correctly model pairwise similarity between items. Since similarity is often a non-linear function, focusing on Euclidean distances between coordinates can be misleading. A summary of similarity can be obtained by assembling a pairwise similarity matrix $\\mathbf{s}$, where the matrix element $s_{ij}$ contains the computed similarity between stimulus $i$ and stimulus $j$. \"Unrolling\" (i.e., reshaping) the matrix into a one dimensional array allows us to easily compare two pairwise similarity matrices.\n",
    "\n",
    "```{note}\n",
    "Depending on the proximity function, some elements of the matrix elements should be omitted from the comparison. For instance, if self-similarity (i.e., diagonal ) values are uninformative, they should be excluded since they may artificially inflate the comparison. In this tutorial, self-similarity is always `1` so we exclude the diagonal elements. If the proximity function is symmetric (`s(z_0,z_1) = s(z_1, z_0)`), it is not necessary to compute both $s_{ij}$ and $s_{ji}. Instead one can compute just the upper (or lower) triangular portion of the similarity matrix. Since our similarity function is symmetric we will only compute the upper triangular portion.\n",
    "```\n",
    "\n",
    "A popular method of comparison is to compute the Pearson correlation between two unrolled similarity matrices. This method of comparison allows us to evaluate the extent that models exhibit *meaningful* differences. This kind of analysis has been around a long time in psychology. For example, Sheperd framed this analysis as focusing on second-order isomorphism (similarity matrix equivalency) rather than focusing on first-order isomorphism (coordinate equivalency) {cite}`Shepard_Chipman_1970`. Today, cognitive scientists often refer to this kind of analysis as representational similarity analysis (RSA) {cite}`Kriegeskorte_Mur_Bandettini_2008`.\n",
    "\n",
    "```{note}\n",
    "There are multiple strategies for comparing similarity matrices, such as canonical correlation analysis (CCA). Each approach has its strengths and weaknesses. Pearson correlation provides a simple, but intuitive strategy. In a research setting, the optimal comparison strategy will depend on the details of the project.\n",
    "```\n",
    "\n",
    "In general, computing a pairwise similarity matrix is expensive in terms of computational and storage costs. While the costs are minimal for this small stimulus set, PsiZ is designed to scale to large problems. For this reason, pairwise similarity is computed using a batch strategy. We start by creating a dataset of index pairs via the `psiz.data.Rate` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemble a TF Dataset of pairwise indices that will be used to compute\n",
    "# \"unrolled\" similarity matrices. Note that we start the indices at `1`\n",
    "# since we are not interested in computing the similarity of the\n",
    "# placeholder `0` index.\n",
    "content_pairs = psiz.data.Rate(\n",
    "    psiz.utils.pairwise_indices(np.arange(n_stimuli) + 1, elements='upper')\n",
    ")\n",
    "# NOTE: We include an placeholder \"target\" component in dataset tuple to\n",
    "# satisfy the assumptions of `predict` method.\n",
    "dummy_outcome = psiz.data.Continuous(np.ones([content_pairs.n_sample, 1]))\n",
    "tfds_pairs = psiz.data.Dataset(\n",
    "    [content_pairs, dummy_outcome]\n",
    ").export().batch(batch_size, drop_remainder=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create a Keras model that predicts pairwise similarity and instantiate an instance for the virtual agent and inferred model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairwiseModel(tf.keras.Model):\n",
    "    \"\"\"A model that outputs similarity between input pairs.\"\"\"\n",
    "\n",
    "    def __init__(self, percept=None, proximity=None, **kwargs):\n",
    "        \"\"\"Initialize.\"\"\"\n",
    "        super(PairwiseModel, self).__init__(**kwargs)\n",
    "        self.percept = percept\n",
    "        self.proximity = proximity\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"Call.\"\"\"\n",
    "        stimuli_axis = 1\n",
    "        z = self.percept(inputs['rate2_stimulus_set'])\n",
    "        z_0 = tf.gather(z, indices=tf.constant(0), axis=stimuli_axis)\n",
    "        z_1 = tf.gather(z, indices=tf.constant(1), axis=stimuli_axis)\n",
    "        return self.proximity([z_0, z_1])\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super(PairwiseModel, self).get_config()\n",
    "        return config\n",
    "\n",
    "\n",
    "virtual_agent_pairwise = PairwiseModel(\n",
    "    percept=virtual_agent_2rank1.percept,\n",
    "    proximity=virtual_agent_2rank1.proximity\n",
    ")\n",
    "inferred_pairwise = PairwiseModel(\n",
    "    percept=inferred_model.percept,\n",
    "    proximity=inferred_model.proximity\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After computing the pairwise similarities for both the `virtual_agent` and `inferred_model`, we compute the Pearson correlation between the two sets of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "Pearson rho: 0.96\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "# The pairwise similarities for the virtual agent.\n",
    "virtual_agent_similarity = virtual_agent_pairwise.predict(tfds_pairs)\n",
    "# The pairwsie similarities for the inferred model.\n",
    "inferred_similarity = inferred_pairwise.predict(tfds_pairs)\n",
    "\n",
    "rho, _ = pearsonr(virtual_agent_similarity, inferred_similarity)\n",
    "print('Pearson rho: {0:.2f}'.format(rho))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The high Pearson correlation reveals that what initially seemed like a notable difference from the perspective of absolute coordinate position and Euclidean distance, is only a minor difference from the perspective of pairwise similarity. The high correlation value also suggests that the embedding solution is stable and that a sufficient amount of data has been used to train the model.\n",
    "\n",
    "```{note}\n",
    "To decide if an embedding is stable, you can use the conservative heuristic of a Pearson correlation of 0.95 or greater. Depending on your research goals, you may need to use a lower or higher threshold. Aiming for a correlation of 1.00 is typically impractical since there are diminishing returns to data collection. If a high Pearson correlation is necessary, you should strongly consider more sophisticated data collection strategies, such as active learning. Strategies like active learning help make the most of your data collection budget.\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The Beginner Tutorial introduced a number of topics:\n",
    "* The general problem PsiZ is designed to solve\n",
    "* Constructing a basic embedding model.\n",
    "* Data prepartion\n",
    "* Model inference\n",
    "* Visualizing an embedding\n",
    "* Checking second-order isomorphism.\n",
    "\n",
    "The Beginner Tutorial employed a number of simplifications to keep the focus on the general organization of PsiZ and avoid clutter from optional features. To learn about additional features, explore the remaining tutorials and example scripts."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. Using only 10% of the data, train a new model. Perform an RSA analysis between the new inferred model amd the virtual agent. What does the RSA analysis suggest? Is 10% of the data sufficient to achieve a stable embedding?\n",
    "2. When instantiating the virtual agent, use `n_dim=3`. Now train three new models: one that uses `n_dim=2`, `n_dim=3`, and `n_dim=4`. Which of the new models has a better test loss? What does this mean from the perspective of selecting a model with the best generalization performance?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<!-- ```{bibliography}\n",
    ":style: plain\n",
    "``` -->"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "65de726c653ecc9d95d3eaef7d4dccdf82ef132dc6a5da9fc48255c203feab6d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('psiz_tf28_py39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
