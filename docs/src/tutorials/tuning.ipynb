{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Tuning\n",
    "\n",
    "## Overview\n",
    "\n",
    "The Tuning tutorial demonstrates how to use PsiZ models with [Keras Tuner](https://keras.io/keras_tuner/) in order to select the best hyperparameters (e.g., embedding dimensionality) for a model.\n",
    "\n",
    "This tutorial uses the `birds-16` dataset introduced in the [Beginner Tutorial](https://psiz.readthedocs.io/en/latest/src/beginner_tutorial/beginner_tutorial.html) and is divided into two parts:\n",
    "\n",
    "1. Model Construction\n",
    "2. Hypertuning\n",
    "\n",
    "If you would like to run this notebook on your local machine, the file is available [here on PsiZ's GitHub](https://github.com/psiz-org/psiz/blob/main/docs/src/tutorials/tuning.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Construction\n",
    "\n",
    "### Preliminaries\n",
    "\n",
    "Let's start by importing the nessary packages, defining some tutorial settings, and preparing the birds-16 dataset. Note that `max_dim=7`, which means we will search models ranging from `n_dim=2` to `n_dim=7`.\n",
    "\n",
    "```{note}\n",
    "Keras Tuner is not automatically installed with PsiZ because there are many hyperparameter tuning packages available and user preferences vary. The next code block includes a line that can be uncommented if you would like to install `keras-tuner` in your active environment.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Summary\n",
      "  n_stimuli: 208\n",
      "  n_trial: 16292\n",
      "\n",
      "Data Split\n",
      "  obs_train: 13033\n",
      "  obs_val: 1629\n",
      "  obs_test: 1630\n"
     ]
    }
   ],
   "source": [
    "# %pip install keras-tuner -q  # Uncomment if you need to install keras-tuner.\n",
    "\n",
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "import keras_tuner as kt\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "import psiz\n",
    "\n",
    "# Optional CUDA settings.\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "# tf.config.run_functions_eagerly(True)\n",
    "\n",
    "# Define where we want our results to be stored.\n",
    "fp_project = Path.home() / Path('psiz_examples', 'tutorial', 'tuning')\n",
    "fp_board = fp_project / Path('logs', 'fit')\n",
    "\n",
    "# Some hardcoded settings for the tutorial.\n",
    "max_epochs = 1000\n",
    "batch_size = 128\n",
    "max_dim = 7\n",
    "executions_per_trial = 1\n",
    "\n",
    "# Directory preparation.\n",
    "fp_project.mkdir(parents=True, exist_ok=True)\n",
    "# Remove existing TensorBoard logs.\n",
    "if fp_board.exists():\n",
    "    shutil.rmtree(fp_board)\n",
    "\n",
    "# Load hosted birds-16 dataset.\n",
    "(obs, catalog) = psiz.datasets.load_dataset('birds-16', verbose=1)\n",
    "\n",
    "# Partition observations.\n",
    "obs_train, obs_val, obs_test = psiz.utils.standard_split(obs)\n",
    "print(\n",
    "    '\\nData Split\\n  obs_train:'\n",
    "    ' {0}\\n  obs_val: {1}\\n  obs_test: {2}'.format(\n",
    "        obs_train.n_trial, obs_val.n_trial, obs_test.n_trial\n",
    "    )\n",
    ")\n",
    "\n",
    "# Convert observations to TF dataset.\n",
    "ds_obs_train = obs_train.as_dataset().shuffle(\n",
    "    buffer_size=obs_train.n_trial, seed=252, reshuffle_each_iteration=True\n",
    ").batch(batch_size, drop_remainder=False)\n",
    "ds_obs_val = obs_val.as_dataset().batch(\n",
    "    batch_size, drop_remainder=False\n",
    ")\n",
    "ds_obs_test = obs_test.as_dataset().batch(\n",
    "    batch_size, drop_remainder=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperpameter Model\n",
    "\n",
    "With the preliminaries out of the way, we can dive in and define our model in a way that is compatible with Keras Tuning. One of the convenient features of Keras Tuning is that it is easy to wrap existing code. So let's start by defining a `build_model` function in the usual manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(n_stimuli, n_obs_train, n_dim):\n",
    "    \"\"\"Build model.\n",
    "\n",
    "    Args:\n",
    "        n_stimuli: Integer indicating the number of stimuli in the\n",
    "            embedding.\n",
    "        n_obs_train: Integer indicating the number of training\n",
    "            observations. Used to determine KL weight for variational\n",
    "            inference.\n",
    "        n_dim: Integer specifying the dimensionality of the\n",
    "            embedding.\n",
    "\n",
    "    Returns:\n",
    "        model: A TensorFlow Keras model.\n",
    "\n",
    "    \"\"\"\n",
    "    prior_scale = .2  # An educated guess.\n",
    "    kl_weight = 1. / n_obs_train\n",
    "\n",
    "    embedding_posterior = psiz.keras.layers.EmbeddingNormalDiag(\n",
    "        n_stimuli + 1, n_dim, mask_zero=True,\n",
    "        scale_initializer=tf.keras.initializers.Constant(\n",
    "            tfp.math.softplus_inverse(prior_scale).numpy()\n",
    "        )\n",
    "    )\n",
    "    embedding_prior = psiz.keras.layers.EmbeddingShared(\n",
    "        n_stimuli + 1, n_dim, mask_zero=True,\n",
    "        embedding=psiz.keras.layers.EmbeddingNormalDiag(\n",
    "            1, 1,\n",
    "            loc_initializer=tf.keras.initializers.Constant(0.),\n",
    "            scale_initializer=tf.keras.initializers.Constant(\n",
    "                tfp.math.softplus_inverse(prior_scale).numpy()\n",
    "            ),\n",
    "            loc_trainable=False,\n",
    "        )\n",
    "    )\n",
    "    stimuli = psiz.keras.layers.EmbeddingVariational(\n",
    "        posterior=embedding_posterior, prior=embedding_prior,\n",
    "        kl_weight=kl_weight, kl_n_sample=30\n",
    "    )\n",
    "\n",
    "    kernel = psiz.keras.layers.DistanceBased(\n",
    "        distance=psiz.keras.layers.Minkowski(\n",
    "            rho_initializer=tf.keras.initializers.Constant(2.),\n",
    "            w_initializer=tf.keras.initializers.Constant(1.),\n",
    "            trainable=False\n",
    "        ),\n",
    "        similarity=psiz.keras.layers.ExponentialSimilarity(\n",
    "            trainable=False,\n",
    "            beta_initializer=tf.keras.initializers.Constant(10.),\n",
    "            tau_initializer=tf.keras.initializers.Constant(1.),\n",
    "            gamma_initializer=tf.keras.initializers.Constant(0.),\n",
    "        )\n",
    "    )\n",
    "    model = psiz.keras.models.Rank(\n",
    "        stimuli=stimuli, kernel=kernel, n_sample=1\n",
    "    )\n",
    "\n",
    "    # Compile settings.\n",
    "    compile_kwargs = {\n",
    "        'loss': tf.keras.losses.CategoricalCrossentropy(),\n",
    "        'optimizer': tf.keras.optimizers.Adam(learning_rate=.001),\n",
    "        'weighted_metrics': [\n",
    "            tf.keras.metrics.CategoricalCrossentropy(name='cce')\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    model.compile(**compile_kwargs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a `build_model` function defined, we can now introduce an additional layer of abstraction that will manage the model's hyperparameters. For this tutorial, we will only consider a setup with one hyperparameter: `n_dim`---the variable that specifies the dimensionality of the embedding.\n",
    "\n",
    "One way to use Keras Tuner is to define a `build_hypmodel` function that *wraps* our `build_model` function. Note that this function behaves somewhat like an anonymous funcation and has three variables that are supplied by the execution context: `max_dim`, `catalog`, and `obs_train`. We use this pattern so that the `build_hypmodel` function has the function signature expected by Keras Tuner.\n",
    "\n",
    "Lastly, we instantiate a `Tuner` object that manages the tuning process and takes `build_hypmodel` as an argument. In this tutorial we use a `RandomSearch` object because we will exhaustively sample all dimensionality values in the specified range. In other scenarios, it may make more sense to use a more intelligent tuner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search space summary\n",
      "Default search space size: 1\n",
      "n_dim (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 2, 'max_value': 7, 'step': 1, 'sampling': None}\n",
      "'executions_per_trial': 1\n"
     ]
    }
   ],
   "source": [
    "def build_hypmodel(hp):\n",
    "    \"\"\"Build hyperparameter model.\n",
    "\n",
    "    Ags:\n",
    "        hp: A kt.Tuner object governing the hyperparameters.\n",
    "\n",
    "    Returns:\n",
    "        model: A model compatible with Keras Tuner.\n",
    "\n",
    "    \"\"\"\n",
    "    n_dim = hp.Int(\"n_dim\", min_value=2, max_value=max_dim, step=1)\n",
    "    return build_model(catalog.n_stimuli, obs_train.n_trial, n_dim)\n",
    "\n",
    "# Build hypertuner that will use for performing multiple restarts.\n",
    "tuner = kt.RandomSearch(\n",
    "    hypermodel=build_hypmodel,\n",
    "    objective=kt.Objective(\"val_loss\", direction=\"min\"),\n",
    "    max_trials=(max_dim - 1),\n",
    "    executions_per_trial=executions_per_trial,\n",
    "    overwrite=True,\n",
    "    directory=fp_project,\n",
    "    project_name=\"logs/tuner\",\n",
    ")\n",
    "# Print out summary of search space.\n",
    "tuner.search_space_summary()\n",
    "print(\"'executions_per_trial': {0}\".format(tuner.executions_per_trial))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypertuning\n",
    "\n",
    "With our hypermodel defined, we create some convenient callbacks (for logging and early stopping) and start the hyperparameter search. A nice thing about Keras Tune is the `search` signature matches the `fit` signature.\n",
    "\n",
    "```{note}\n",
    "The `TensorBoard` callback saves log files in a format that allows us better understand how different hyperparameters impact model performance. All you need to do is launch TensorBoard in the standard way and you will see an additional \"HPARAMS\" tab that allows you to explore the hyperparameter search results. For more information, check out the (TensorBoard Documentation)[https://www.tensorflow.org/tensorboard] and the tutorial on using (TensorBoard with Keras Tuner)[https://keras.io/guides/keras_tuner/visualize_tuning/]).\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 369: early stopping\n",
      "Epoch 470: early stopping\n",
      "Epoch 449: early stopping\n",
      "Epoch 469: early stopping\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "Results summary\n",
      "Results in /home/brett/psiz_examples/tutorial/tuning/logs/tuner\n",
      "Showing 10 best trials\n",
      "<keras_tuner.engine.objective.Objective object at 0x7ff7a4516820>\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "n_dim: 3\n",
      "Score: 1.9470715522766113\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "n_dim: 4\n",
      "Score: 1.9478240013122559\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "n_dim: 5\n",
      "Score: 1.9496349096298218\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "n_dim: 6\n",
      "Score: 1.9694207906723022\n"
     ]
    }
   ],
   "source": [
    "# Define callbacks.\n",
    "cb_board = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir=fp_board, write_graph=False\n",
    ")\n",
    "cb_early = tf.keras.callbacks.EarlyStopping(\n",
    "    'loss', patience=100, mode='min', restore_best_weights=False,\n",
    "    verbose=1\n",
    ")\n",
    "callbacks = [cb_board, cb_early]\n",
    "\n",
    "# Start the hyperparameter search. The default settings will take awhile.\n",
    "tuner.search(\n",
    "    x=ds_obs_train, validation_data=ds_obs_val, epochs=max_epochs,\n",
    "    callbacks=callbacks, verbose=0\n",
    ")\n",
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running out hyperparameter search, we can load the best performing model and run it on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best hps: {'n_dim': 3}\n",
      "Test Set Performance: {'loss': 1.7893924713134766, 'cce': 1.6967400312423706}\n"
     ]
    }
   ],
   "source": [
    "# Retrieve best hyperparameters from search.\n",
    "best_hps = tuner.get_best_hyperparameters()\n",
    "print('best hps: {}'.format(best_hps[0].values))\n",
    "# Retrieve best model from search.\n",
    "best_model = tuner.get_best_models()[0]\n",
    "\n",
    "# Evaluate on test set\n",
    "metrics_test = best_model.evaluate(ds_obs_test, verbose=0, return_dict=True)\n",
    "print('Test Set Performance: {}'.format(metrics_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of the search indicate that the best dimensionality for the `birds-16` dataset is `3`. If you would like to increase your confidence in this result you could increase the variable `executions_per_trials` or set up a cross-validation procedure that uses different training splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This tutorial demonstrated how to use `keras-tuner` in order to conduct a hyperparameter search and identify the optimal dimensionality for the `birds-16` dataset. This search can easily be expanded to search over other hyperparameters, such as the optimizer settings."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "65de726c653ecc9d95d3eaef7d4dccdf82ef132dc6a5da9fc48255c203feab6d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('psiz_tf28_py39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
